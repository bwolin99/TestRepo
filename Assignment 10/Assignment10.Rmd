---
title: "Assignment 10"
author: "Ben Wolin"
date: "2024-11-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction
In this assignment we will be performing a sentiment analysis of Jane Austin books that is taken from Text Mining with R, Chapter 2 looks at Sentiment Analysis. We will then be performing a similar analysis of the book Dracula by Bram Stoker.

## Jane Austing Analysis
Here we load in our needed packages and assign the Jane Austin books to a dataframe tidy_books.
---
```{r}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(ggplot2)
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

Here we get the counts of the words associated with the sentiment "Joy" from the book "Emma" using the nrc sentiment lexicon.
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

Here we are making a dataframe with the amount of words with a negative and positive sentiment from the "Bing" library per 80 lines. Then a plot is made for each Jane Austin book showing how the sentiment changes as the books go on. We can use this to pick out particularly negative or positive sections of the text.
```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

Over the next few code block we will generate 3 graphs showing the sentiment over time of the book Pride and Prejudice. These will be comparing the results when using 3 different sentiment lexicons.
```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice
```
```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```
```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

Now we will make 2 histigrams comparing the counts of postive and negative words in the Jane Austin books.
```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
We can see in the negative chart that the word "miss" is very common. This word has a negative connotation in the lexicon, however, this is used in these books as a title for young, unmarried women.Without this outlier it is clear that these books are overwhelmingly positive.

Now we will generate 2 word clouds, one with the top 100 words in the Jane Austin books, the other differentiating the words between positive and negative.
```{r}
library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
```{r}
library(reshape2)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

Next we will be using the unnest_tokens() to split up the books by chapter.
```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

We can then use a sentiment lexicon to determine the most negative chapters in Jane Austins books by their ratio of negative words.
```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()
```

## Dracula Sentiment Analysis
No we will be loughran and nrc sentiment lexicons to analyze the book Dracula by Bram Stoker.

First, we will bring the novel into our environment from a github repository and putting it into a dataframe
```{r}
book <- read.delim('https://raw.githubusercontent.com/bwolin99/TestRepo/refs/heads/main/Assignment%2010/Dracula.txt',row.names = NULL)
dracula <- data.frame('text' = book, 'book' = 'Dracula')
names(dracula)[names(dracula) == 'row.names'] <- 'text'
```

Now we will create a chart showing the sentiment of Dracula over time.
```{r}
dracula_chapters <- dracula %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

dracula_sentiment <- dracula_chapters %>%
  inner_join(get_sentiments("loughran")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(dracula_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```
As we can see Dracula contains an overwhelming amount of negative sentiment. This a stark difference from the Jane Austin books which is to be expected.

Now we will chart the most common words for each sentiment in the lexicon. This will show us if there are any anomalies, such as "miss" in the Jane Austin books.

```{r}
dracula_counts <- dracula_chapters %>%
  inner_join(get_sentiments("loughran")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

dracula_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
This graph shows us that the most common words show uncertainty, which makes sense since Dracula is somewhat of a mystery where the characters are unsure of the supernatural causes of there predicament. Something surprising here is how common the word "good" is in the novel considering the sentiment over time chart. Out of all the positive and negative words "good" is the most common. This most likely means the lower ranked words in the negative sentiment category are much more common than there positive counterparts. This should be best visualized by a wordcloud.

```{r}
library(wordcloud)
library(reshape2)
dracula_chapters %>%
  inner_join(get_sentiments("loughran")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20","red","blue","cyan","pink", "gray80"),
                   max.words = 100)
```
From this wordcloud we can see that negative and constraining words are the most common sentiments in Dracula. This is then followed closely behind by uncertainty and postive words. A very word is "whilst" which is a a word commonly used to describe something happening at the same time as something else, typically with a slight negative tone.
